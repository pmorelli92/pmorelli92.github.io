<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Dev &amp; Chill</title>
        <link>https://devandchill.com/posts/</link>
        <description>Recent content in Posts on Dev &amp; Chill</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>Pablo Morelli</copyright>
        <lastBuildDate>Mon, 24 Jun 2019 17:22:29 +0200</lastBuildDate>
        <atom:link href="https://devandchill.com/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>NET Core 3.0 - Publish single file binary on alpine container</title>
            <link>https://devandchill.com/posts/2019/06/net-core-3.0-publish-single-file-binary-on-alpine-container/</link>
            <pubDate>Mon, 24 Jun 2019 17:22:29 +0200</pubDate>
            
            <guid>https://devandchill.com/posts/2019/06/net-core-3.0-publish-single-file-binary-on-alpine-container/</guid>
            <description>The newly released dotnet 3 preview 6 has an interesting feature called publish single file where an application can be published as a self contained app that will extract and run on the platform it was compiled for.
In order to add this feature in our .csproj, add the following lines:
&amp;lt;PropertyGroup&amp;gt; &amp;lt;TargetFramework&amp;gt;netcoreapp3.0&amp;lt;/TargetFramework&amp;gt; &amp;lt;PublishTrimmed&amp;gt;true&amp;lt;/PublishTrimmed&amp;gt; &amp;lt;PublishReadyToRun&amp;gt;true&amp;lt;/PublishReadyToRun&amp;gt; &amp;lt;PublishSingleFile&amp;gt;true&amp;lt;/PublishSingleFile&amp;gt; &amp;lt;CrossGenDuringPublish&amp;gt;false&amp;lt;/CrossGenDuringPublish&amp;gt; &amp;lt;/PropertyGroup&amp;gt;  There are some flags that may sound weird, let&amp;rsquo;s go through them:</description>
            <content type="html"><![CDATA[

<p>The newly released <code>dotnet 3 preview 6</code> has an interesting feature called <code>publish single file</code> where an application can be published as a <code>self contained app</code> that will <strong>extract and run</strong> on the platform it was compiled for.</p>

<p>In order to add this feature in our <code>.csproj</code>, add the following lines:</p>

<pre><code class="language-xml">&lt;PropertyGroup&gt;
  &lt;TargetFramework&gt;netcoreapp3.0&lt;/TargetFramework&gt;
  &lt;PublishTrimmed&gt;true&lt;/PublishTrimmed&gt;
  &lt;PublishReadyToRun&gt;true&lt;/PublishReadyToRun&gt;
  &lt;PublishSingleFile&gt;true&lt;/PublishSingleFile&gt;
  &lt;CrossGenDuringPublish&gt;false&lt;/CrossGenDuringPublish&gt;
&lt;/PropertyGroup&gt;
</code></pre>

<p>There are some flags that may sound weird, let&rsquo;s go through them:</p>

<ul>
<li><code>PublishTrimmed</code> indicates to remove all the dotnet dependencies that are not being used by our code. When doing this in a real world application, it is very important to test it afterwards in case some dependency that is referenced by code (for example using reflection) is being trimmed as well.</li>
<li><code>PublishReadyToRun</code> it is supposed to make the start time of our application faster, but in the process it will make the binary heavier.</li>
<li><code>CrossGenDuringPublish</code> is a workaround that needs to be present for indicating the machine that is running the publish, that it should not bother with compilation for other RID (more about this below)</li>
</ul>

<p>After these flags had been set, the application is runnable using:</p>

<pre><code>dotnet publish -r osx-x64 -c Release -o ./deploy
./deploy/&lt;APP_NAME&gt; &lt;- Execute the application
</code></pre>

<p>When publishing a single file, don&rsquo;t forget to add the <a href="https://docs.microsoft.com/en-us/dotnet/core/rid-catalog">runtime identifier catalog (RID)</a> corresponding to the platform where the application is going to be run. For example: <code>-r osx-x64</code></p>

<p>With the basics of this feature already covered, let&rsquo;s move to the important bit.</p>

<h2 id="deploy-application-on-an-alpine-based-container">Deploy application on an Alpine based container</h2>

<p>One important win using this approach is that the published binary does not need to be run in a container that has dotnet runtime installed. But one caveat for consideration is that those internal dependencies used under the hood by dotnet are still needed. There are two options:</p>

<p>Use plain alpine and install dotnet internal dependencies:</p>

<pre><code class="language-Dockerfile">FROM alpine
RUN apk update &amp;&amp; apk add libstdc++ &amp;&amp; apk add libintl
</code></pre>

<p>Use <code>runtime-deps</code> alpine image from <a href="https://hub.docker.com/_/microsoft-dotnet-core-runtime-deps/">Microsoft</a>:</p>

<blockquote>
<p>This image contains the native dependencies needed by .NET Core. It does not include .NET Core. It is for self-contained applications.</p>
</blockquote>

<pre><code class="language-Dockerfile">FROM mcr.microsoft.com/dotnet/core/runtime-deps:3.0.0-preview6-alpine3.9
</code></pre>

<p>With that being said, the next is straightforward:</p>

<pre><code class="language-Dockerfile">FROM mcr.microsoft.com/dotnet/core/sdk:3.0.100-preview6-alpine3.9 AS build

WORKDIR /app

COPY . .

RUN dotnet publish -r linux-musl-x64 -c Release -o ./deploy

FROM mcr.microsoft.com/dotnet/core/runtime-deps:3.0.0-preview6-alpine3.9

COPY --from=build /app/deploy ./app

ENV ASPNETCORE_URLS http://*:5000

EXPOSE 5000

ENTRYPOINT [&quot;./app/&lt;APP_NAME&gt;&quot;]
</code></pre>

<p>Basically the <code>Dockerfile</code> is publishing the application (as a binary) on an alpine container with dotnet sdk installed. This is important because the <code>builder</code> has to use the same architecture than the runner. It is not possible to build a <code>linux-musk-x64</code> binary on a windows container.</p>

<p>Then the result files (the binary and a single DLL) are copied to an alpine container only having dotnet runtime dependencies.</p>

<p>At the end the <code>ENTRYPOINT</code> line is executed. Check that this one is not doing <code>dotnet app/&lt;APP_NAME&gt;</code> but just <code>./app/&lt;APP_NAME&gt;</code></p>

<p>I did a sample API with this approach, <a href="https://github.com/pmorelli92/dotnet3.executable.api">which can be found my github</a>. The container weighted <code>91MB</code>, used <code>30MB RAM</code> and <code>0.08% CPU</code>. Awesome, right?</p>
]]></content>
        </item>
        
        <item>
            <title>Sign GitHub commits (using Mac)</title>
            <link>https://devandchill.com/posts/2019/06/sign-github-commits-using-mac/</link>
            <pubDate>Sun, 09 Jun 2019 20:22:29 +0200</pubDate>
            
            <guid>https://devandchill.com/posts/2019/06/sign-github-commits-using-mac/</guid>
            <description>Generally speaking, most organisations do not check if a commit is signed or not. Even worse, more companies do not care about this. It might subjective but I like to have the verified badge.
 If you commit from GitHub, for example: adding or editing a README, you will get the verified bagde. The same will happen with the client GitKraken, but if you commit using the git cli this will not happen unless you configure it.</description>
            <content type="html"><![CDATA[

<p>Generally speaking, most organisations do not check if a commit is signed or not. Even worse, more companies do not care about this. It might subjective but I like to have the <code>verified</code> badge.</p>


    <figure class="center" >
        <img src="/posts/images/verified-commit.png"   />

        
    </figure>



<p>If you commit from GitHub, for example: adding or editing a README, you will get the verified bagde. The same will happen with the client <code>GitKraken</code>, but if you commit using the <code>git cli</code> this will not happen unless you configure it.</p>

<p>Now, I will assume that you are using a mac but in case you do not, you can google yourself some of the installation steps.</p>

<h2 id="installing-gpg">Installing GPG</h2>

<p>In order to sign a commit you will need to generate a key, and for this you are going to use <a href="https://gnupg.org/">GPG</a>. But first, you need brew to be installed, if you do not have it yet, it is a must have package manager for mac that you can download <a href="https://brew.sh/">here</a>.</p>

<pre><code>&gt; brew install gnupg
&gt; brew install pinentry-mac
&gt; echo &quot;pinentry-program /usr/local/bin/pinentry-mac&quot; &gt;&gt; ~/.gnupg/gpg-agent.conf
&gt; killall gpg-agent
</code></pre>

<p><a href="https://www.gnupg.org/related_software/pinentry/index.en.html">Pinentry</a> as it is stated on the GPG site it is a collection of dialogs that allows you, for example, to input the passphrase of the private key. In order for this to work we need to indicate the path where it is installed into the GPG configuration and then kill the service (just in case) for it to take the changes.</p>

<h2 id="generating-a-set-of-keys">Generating a set of keys</h2>

<pre><code>&gt; gpg --full-generate-key
(you can customise the key creation or just press enter to use the defaults)
</code></pre>

<p>Will generate a private key from which you are going to generate and export the public key. The id of the key will be shown when you generate the key or you can fetch it by running:</p>

<pre><code>&gt; gpg --list-secret-keys
</code></pre>

<p>And you will get something like this:</p>

<pre><code>sec   rsa2048 2019-06-07 [SC]
      KEY_ID_HERE
uid   [  absolut ] Pablo Morelli &lt;devandchill@gmail.com&gt;
ssb   rsa2048 2019-06-07 [E]
</code></pre>

<p>After you have your ID, execute:</p>

<pre><code>&gt; gpg --export --armor YOUR_KEY_ID 2&gt; cat
</code></pre>

<p>You will see the public key on the terminal, now you can copy it and go to GitHub.</p>

<h2 id="adding-public-key-to-github-account">Adding public key to GitHub account</h2>

<p>Go to <code>Settings -&gt; SSH and GPG Keys -&gt; New GPG Key</code> and paste the content of your key and after submiting you will see something like this.</p>


    <figure class="center" >
        <img src="/posts/images/gpg-github.png"   />

        
    </figure>



<h2 id="doing-your-first-verified-commit">Doing your first verified commit</h2>

<p>When you work with the <code>git cli</code> there is a huge probability that you work with different accounts (for example one for work and one personal) and/or that you have more than one GPG key on your system.
To avoid problems with git not knowing which configuration to pick you will indicate that for the repo where you are standing or for all the repos you will use the GPG key that contains a certain ID.</p>

<pre><code>&gt; git config --local gpg.program &quot;gpg&quot;
&gt; git config --local user.signingKey &quot;YOUR_KEY_ID_INSIDE_THE_QUOTES&quot; // will apply to the working directory

&gt; git config --global gpg.program &quot;gpg&quot;
&gt; git config --global user.signingKey &quot;YOUR_KEY_ID_INSIDE_THE_QUOTES&quot; // will apply to the all the repositories except when overriden
</code></pre>

<p>You are almost ready to go, but you need to know that by default <code>git commit</code> does not sign the commits unless you specify the argument <code>-S</code>. So you have two options:</p>

<pre><code>&gt; git commit -S -m &quot;My commit message&quot;
</code></pre>

<p>Or you can setup automatic signing for the repository or all the repositories.</p>

<pre><code>&gt; git config --local commit.gpgsign &quot;true&quot;

&gt; git config --global commit.gpgsign &quot;true&quot; // if you want to set up for all the repositories
</code></pre>

<p>If you did the step above, the argument <code>-S</code> is no longer needed.</p>

<h2 id="checking-git-configuration">Checking git configuration</h2>

<p>When you are unsure about your configuration you can execute <code>git config --local -l</code> to list the configuration for the working directory and you should see something like this:</p>

<pre><code>gpg.program=gpg
commit.gpgsign=true
user.signingkey=YOUR_KEY_ID
</code></pre>

<h2 id="back-up-your-private-key">Back up your private key</h2>

<p>Keys are passwords, so you should save them somewhere safe and revoke + change periodically.
It is trivial to revoke and to generate a new key for GitHub but it may be troublesome for some other systems that uses your public key.
Now lets image that you change your computer, you wont be able to sign because you don&rsquo;t have your key; in this case a backup is recommended.</p>

<pre><code>gpg --export-secret-keys YOUR_KEY_ID &gt; some_private_key_name
</code></pre>

<p>Now you can save this file to some secure vault, and if you ever need to import it on a new machine, execute:</p>

<pre><code>gpg --import some_private_key_name
</code></pre>

<h2 id="bonus-track">Bonus track</h2>

<p>As I mentioned before, some times when you work using the <code>git cli</code> you end up mixing accounts, and it could happen to commit like this:</p>


    <figure class="center" >
        <img src="/posts/images/incorrect-user.png"   />

        
    </figure>



<p>You can of course leave it like that or do a rebase of the commit; but you can also check first using <code>git config --local -l</code> which is the <code>user.name</code> and <code>user.email</code> for that working directory.
In case the email does not match with the email linked in GitHub you can change it doing:</p>

<pre><code>&gt; git config --local user.name &quot;GitHub name&quot;
&gt; git config --local user.email &quot;my.github@email.com&quot;
</code></pre>

<p>I hope you liked the post, if so, don&rsquo;t forget to share it so this can reach to more people :)</p>
]]></content>
        </item>
        
        <item>
            <title>Word Counter: Sync or Concurrent?</title>
            <link>https://devandchill.com/posts/2019/04/word-counter-sync-or-concurrent/</link>
            <pubDate>Wed, 10 Apr 2019 20:59:47 +0200</pubDate>
            
            <guid>https://devandchill.com/posts/2019/04/word-counter-sync-or-concurrent/</guid>
            <description>One day I was given the task of doing a whiteboard word counter algorithm. The requirements were:
 Print on console all the words and the quantity of times they appear on a 40.000 lines input. Make it as fast as possible.  For a start we can do it as simple as possible instead, and then try to see whether we can make it faster:
func main() { fmt.</description>
            <content type="html"><![CDATA[

<p>One day I was given the task of doing a whiteboard <code>word counter</code> algorithm. The requirements were:</p>

<ul>
<li>Print on console all the words and the quantity of times they appear on a 40.000 lines input.</li>
<li>Make it as fast as possible.</li>
</ul>

<p>For a start we can do it as simple as possible instead, and then try to see whether we can make it faster:</p>

<pre><code class="language-go">func main() {
	fmt.Println(wordCounter())
}

func wordCounter() map[string]int {
	b, _ := ioutil.ReadFile(&quot;input.txt&quot;)
	inputText := string(b)
	mostFrequent := make(map[string]int)
	removeSpecial := regexp.MustCompile(`(?m)[^a-z]`)

	for _, w := range strings.Split(inputText, &quot; &quot;) {
		w = strings.ToLower(w)
		w = removeSpecial.ReplaceAllString(w, &quot;&quot;)
		mostFrequent[w] = mostFrequent[w] + 1
	}

	return mostFrequent
}
</code></pre>

<p>If I had to explain the code it will be like this:</p>

<ol>
<li>Read the file and put it on a string variable.</li>
<li>Create the map structure that will hold a key (word) and a value (quantity)</li>
<li>Create a regular expression for removing characters like <code>, ! ?</code></li>
<li>For each word in the text (splitting by spaces)</li>
<li>Lowercase it + execute the regex + add 1 to the map quantity for that word</li>
<li>When we finish, we print to the console.</li>
</ol>

<p>This works fine, but we are using Go and every post you visit is probably talking about <code>goroutines</code> and how easy is to achieve concurrency. So what if I do the following?</p>

<pre><code class="language-go">func main() {
	fmt.Println(wordCounterConcurrent())
}

func wordCounterConcurrent() map[string]int {
	b, _ := ioutil.ReadFile(&quot;input.txt&quot;)
	inputText := string(b)
	mostFrequent := make(map[string]int)
	removeSpecial := regexp.MustCompile(`(?m)[^a-z]`)

	wg := sync.WaitGroup{}

	for _, w := range strings.Split(inputText, &quot; &quot;) {
		wg.Add(1)
		go func(w2 string) {
			defer wg.Done()
			w2 = strings.ToLower(w2)
			w2 = removeSpecial.ReplaceAllString(w2, &quot;&quot;)
			mostFrequent[w2] = mostFrequent[w2] + 1
		}(w)
	}

	wg.Wait()
	return mostFrequent
}
</code></pre>

<p>The differences in this snippet and the first one are the following:</p>

<ol>
<li>We defined a <code>WaitGroup</code> that will allow us to:

<ul>
<li>Add a counter every time a goroutine is fired.</li>
<li>Block the main goroutine with the <code>wg.Wait()</code> until all the other goroutines are finished.</li>
<li><em>Decrease</em> the counter executing <code>wg.Done()</code> every time a goroutine is finished.</li>
</ul></li>
<li>We put the functionality that is going to be executed for each word in an <code>anonymous function</code>. So now, we can use the word <code>go</code> to execute that function in a different goroutine.</li>
</ol>

<p>Does this work? Unfortunately no. If we execute the following:</p>

<pre><code>&gt; go run main.go --race
fatal error: concurrent map read and map write
</code></pre>

<p>Basically the race detector is telling us that some of the goroutines can execute this line at the same moment:</p>

<pre><code class="language-go">mostFrequent[w2] = mostFrequent[w2] + 1
</code></pre>

<p>If the map does not want to be written at the same time by two different goroutines we should use some sort of <code>queue</code> and then consume the values in a single threaded process. Fortunately, Go have us covered with <code>channels</code>.</p>

<p>Doing the following modifications should work:</p>

<pre><code class="language-go">func main() {
	fmt.Println(wordCounterConcurrent())
}

func wordCounterConcurrent() map[string]int {
    runtime.GOMAXPROCS(4) //Make sure we use all processors
	b, _ := ioutil.ReadFile(&quot;input.txt&quot;)
	inputText := string(b)
	mostFrequent := make(map[string]int)
	removeSpecial := regexp.MustCompile(`(?m)[^a-z]`)

	doneChan := make(chan bool)
	wordsChan := make(chan string)

	go func() {
		for {
			select {
			case w := &lt;-wordsChan:
				mostFrequent[w] = mostFrequent[w] + 1
			case &lt;-doneChan:
				return
			}
		}
	}()

	wg := sync.WaitGroup{}

	for _, w := range strings.Split(inputText, &quot; &quot;) {
		wg.Add(1)
		go func(w1 string) {
			defer wg.Done()
			w1 = strings.ToLower(w1)
			w1 = removeSpecial.ReplaceAllString(w1, &quot;&quot;)
			wordsChan &lt;- w1
		}(w)
	}

	wg.Wait()
	doneChan &lt;- true
	return mostFrequent
}
</code></pre>

<p>But now the code growth a lot, lets look at it by parts:</p>

<p>In order to declare channels we use the following syntax</p>

<pre><code class="language-go">doneChan := make(chan bool)
wordsChan := make(chan string)
</code></pre>

<p>Now, if we go to the words goroutines we can see that instead of performing operations using the map we are just pushing a value into a channel.</p>

<pre><code class="language-go">wordsChan &lt;- w1
</code></pre>

<p>The operations using the map, then will be performed when we read a value from the channel (blocking action since it is not a buffered channel)</p>

<pre><code class="language-go">w := &lt;-wordsChan:
</code></pre>

<p>For that we have the following:</p>

<pre><code class="language-go">go func() {
    for {
        select {
        case w := &lt;-wordsChan:
            mostFrequent[w] = mostFrequent[w] + 1
        case &lt;-doneChan:
            return
        }
    }
}()
</code></pre>

<p>This piece of code will read from the <code>wordsChannel</code> and perform the map operation; and also read from the <code>doneChannel</code> to stop the goroutine.</p>

<p>The message to the <code>doneChannel</code> will only be submitted after all words goroutines are finished.</p>

<pre><code class="language-go">wg.Wait()
doneChan &lt;- true
</code></pre>

<p>Now if we run this algorithm we will see that is not failing anymore. Let&rsquo;s then move to benchmarking.</p>

<h2 id="benchmark">Benchmark</h2>

<p>We are going to do this in two different ways: using the command-line utility time and executing an idiomatic Go benchmark.</p>

<pre><code>&gt; go test -run=XXX -bench=.
</code></pre>

<pre><code>BenchmarkWordCounter-8                       100          16685557 ns/op
BenchmarkWordCounterConcurrent-8              30          46959704 ns/op
</code></pre>

<p>This is telling us that the sync version executed <code>100 times</code> and each execution took approx <code>16685557 nanoseconds</code>, meanwhile in the same time the concurrent version only got executed <code>30 times</code> because each execution took approx <code>46959704 nanoseconds</code>.</p>

<p>Wait, What?</p>

<p>Let&rsquo;s do it with this:</p>

<pre><code>&gt; go build
&gt; time ./go-word-counter
</code></pre>

<pre><code>(SYNC)
./go-word-counter  0.02s user 0.00s system 57% cpu 0.048 total
(CONCURRENT)
./go-word-counter  0.31s user 0.04s system 362% cpu 0.096 total
</code></pre>

<p>So the sync version takes approx <code>0.048</code> and <code>57% cpu</code> and the concurrent one <code>0.096</code> and <code>362% cpu</code>.</p>

<p>We are sure that the function is running concurrently (hence the CPU usage), but shouldn&rsquo;t that mean it has to run faster? NO.</p>

<p>If we look at the code we can see that the parts that get executed inside the word goroutines takes less to execute than all the orchestration needed for pulling a word from a channel.</p>

<p>Things would be different if we add a sleep like this:</p>

<pre><code class="language-go">w1 = strings.ToLower(w1)
w1 = removeSpecial.ReplaceAllString(w1, &quot;&quot;)
time.Sleep(200)
</code></pre>

<p>And then the benchmark:</p>

<pre><code>BenchmarkWordCounter-8                        10         171650568 ns/op
BenchmarkWordCounterConcurrent-8              20          54229310 ns/op
</code></pre>

<p>In this scenario we can clearly see how the concurrent way outperforms the sync one.</p>

<p>So we started with a single goroutine algorithm, we evolved it to run using more than one goroutine and having an orchestration for the algorithm to work. At the end we did a benchmark and compared the results.</p>

<p>The value we can get out of this is to avoid <code>premature optimization</code>, how to <code>KISS (Keep it simple and stupid)</code>; and the most important one: We learned that concurrent flows are beneficial when the code inside the goroutine is CPU/time intensive.</p>

<p>👉 <a href="https://github.com/pmorelli92/go-word-counter">Repository with source files used</a> 👈</p>
]]></content>
        </item>
        
        <item>
            <title>Go Modules: Working outside GOPATH</title>
            <link>https://devandchill.com/posts/2019/03/go-modules-working-outside-gopath/</link>
            <pubDate>Wed, 13 Mar 2019 17:07:15 +0100</pubDate>
            
            <guid>https://devandchill.com/posts/2019/03/go-modules-working-outside-gopath/</guid>
            <description>When I started learning Golang (a couple of months ago) one of the things that I concerned most with was the project structure.
It may sound irrelevant to bother with this when you are learning the language; but, every time I am browsing .net core applications or libraries on GitHub, I find it extremely unappealing when certain projects are not structured in a conventional way.
My starting point was the following talk:</description>
            <content type="html"><![CDATA[<p>When I started learning Golang (a couple of months ago) one of the things that I concerned most with was the <strong>project structure</strong>.</p>

<p>It may sound irrelevant to bother with this when you are learning the language; but, every time I am browsing .net core applications or libraries on GitHub, I find it extremely unappealing when certain projects are not structured in a conventional way.</p>

<p>My starting point was the following talk:</p>


<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/oL6JBUk6tj0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>


<p>Being that I did not know how to manage dependencies, I started investigating and saw that lots of people were using <code>dep</code>, and how bad could it be?</p>

<p>I started building a sample app that that could be containerised (again, just to see how the Dockerfile would look like), and the following happened:</p>

<pre><code class="language-dockerfile">FROM golang:1.11 AS builder

COPY . /go/src/github.com/pmorelli92/go-ddd-cqrs/
WORKDIR /go/src/github.com/pmorelli92/go-ddd-cqrs/

RUN set -x &amp;&amp; go get github.com/golang/dep/cmd/dep &amp;&amp; dep ensure -v

RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -o goapp ./cmd/server/main.go

FROM scratch
WORKDIR /root/
COPY --from=builder /go/src/github.com/pmorelli92/go-ddd-cqrs/goapp .

EXPOSE 8080
ENTRYPOINT [&quot;./goapp&quot;]
</code></pre>

<p>That Dockerfile, as ugly as it looks, was the result of some iterations were I was getting path errors, dependency errors, and others. The constraint of only working on <code>GOPATH</code> gave me headaches and I had to use a really bad <code>WORKDIR</code> as you can see above.</p>

<p>But then after some searching, I came across that <a href="https://github.com/golang/go/wiki/Modules">GO Modules</a> was supported from version 1.11, so I decided to give it a try.</p>

<p>And this is how the Dockerfile looks like now:</p>

<pre><code class="language-dockerfile">FROM golang:1.12 AS builder

WORKDIR /app

COPY go.mod .
COPY go.sum .

RUN go mod download

COPY . .

RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -o goapp ./cmd/main.go

FROM scratch
COPY --from=builder /app/goapp .

EXPOSE 8080
ENTRYPOINT [&quot;./goapp&quot;]
</code></pre>

<p>We can achieve the same using <code>golang:1.11</code> but we wil have to execute an additional step <code>ENV GO111MODULE=on</code> before <code>RUN go mod download</code>.</p>

<p>So how do we achieve this? Easily!</p>

<pre><code class="language-go">mkdir my-demo // Create a project folder, or clone a github repository
cd my-demo
touch main.go // Create a file if we are starting the project
go mod init my-demo // This will initiate the module
</code></pre>

<p>After doing this, we can proceed installing dependencies by doing <code>go get -u &lt;path&gt;</code></p>

<pre><code class="language-go">go get -u github.com/labstack/echo/v4
</code></pre>

<p>This will be tracked on the <code>go.mod</code> where it will state all the dependencies required for the project:</p>

<pre><code class="language-go">my-demo &gt; cat go.mod
module my-demo

go 1.12

require (
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/labstack/echo/v4 v4.0.0 // indirect
	github.com/mattn/go-colorable v0.1.1 // indirect
	github.com/mattn/go-isatty v0.0.7 // indirect
	github.com/stretchr/objx v0.1.1 // indirect
	github.com/valyala/fasttemplate v1.0.0 // indirect
	golang.org/x/crypto v0.0.0-20190313024323-a1f597ede03a // indirect
	golang.org/x/sys v0.0.0-20190312061237-fead79001313 // indirect
)
</code></pre>

<p>And we are done! Now we can edit the <code>main.go</code> to this:</p>

<pre><code class="language-go">package main

import &quot;net/http&quot;
import &quot;github.com/labstack/echo/v4&quot;

func main() {
	e := echo.New()
	e.GET(&quot;/hello&quot;, func(c echo.Context) error {
		return c.JSON(http.StatusOK, &quot;hey there&quot;)
	})

	_ = e.Start(&quot;:8080&quot;)
}
</code></pre>

<p>If now we look again at <code>go.mod</code> we should see that the module <code>github.com/labstack/echo/v4 v4.0.0</code> is no longer indirect, that means that it is being actively used on the code.</p>

<p><strong>Things to remember</strong></p>

<ul>
<li>Both go.mod and <code>go.sum</code> have to be committed.</li>
<li>When building a docker file, in order to take advantage of the layered cache steps, we should first copy <code>go.mod</code> and <code>go.sum</code> and then executing <code>RUN go mod download</code>; after that, we can safely copy our code.</li>
</ul>
]]></content>
        </item>
        
    </channel>
</rss>
